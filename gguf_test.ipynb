{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from src.GGUFBot import GGUFBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: additional 3 GGUFs metadata loaded.\n",
      "llama_model_loader: loaded meta data with 29 key-value pairs and 563 tensors from model/Mixtral-8x22B-Instruct-v0.1.Q5_K_M-00001-of-00004.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models--mistralai--Mixtral-8x22B-Inst...\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 56\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 65536\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 6144\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 16384\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 48\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  11:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  13:                           llama.vocab_size u32              = 32768\n",
      "llama_model_loader: - kv  14:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,32768]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,32768]   = [3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{bos_token}}{% for message in messag...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  26:                                   split.no u16              = 0\n",
      "llama_model_loader: - kv  27:                                split.count u16              = 4\n",
      "llama_model_loader: - kv  28:                        split.tensors.count i32              = 563\n",
      "llama_model_loader: - type  f32:  113 tensors\n",
      "llama_model_loader: - type  f16:   56 tensors\n",
      "llama_model_loader: - type q8_0:  112 tensors\n",
      "llama_model_loader: - type q5_K:  253 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 1027/32768 vs 259/32768 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32768\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 65536\n",
      "llm_load_print_meta: n_embd           = 6144\n",
      "llm_load_print_meta: n_head           = 48\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 56\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 6\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 16384\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 65536\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8x22B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 140.63 B\n",
      "llm_load_print_meta: model size       = 93.11 GiB (5.69 BPW) \n",
      "llm_load_print_meta: general.name     = models--mistralai--Mixtral-8x22B-Instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 781 '<0x0A>'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: no\n",
      "ggml_cuda_init: found 8 CUDA devices:\n",
      "  Device 0: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 1: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 2: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 3: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 4: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 5: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 6: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "  Device 7: NVIDIA A100-SXM4-80GB, compute capability 8.0, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    2.51 MiB\n",
      "llm_load_tensors: offloading 56 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 57/57 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   132.00 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 12238.73 MiB\n",
      "llm_load_tensors:      CUDA1 buffer size = 13375.12 MiB\n",
      "llm_load_tensors:      CUDA2 buffer size = 11830.73 MiB\n",
      "llm_load_tensors:      CUDA3 buffer size = 11728.73 MiB\n",
      "llm_load_tensors:      CUDA4 buffer size = 11728.73 MiB\n",
      "llm_load_tensors:      CUDA5 buffer size = 11830.73 MiB\n",
      "llm_load_tensors:      CUDA6 buffer size = 11830.73 MiB\n",
      "llm_load_tensors:      CUDA7 buffer size = 10647.87 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4000\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   109.38 MiB\n",
      "llama_kv_cache_init:      CUDA1 KV buffer size =   125.00 MiB\n",
      "llama_kv_cache_init:      CUDA2 KV buffer size =   109.38 MiB\n",
      "llama_kv_cache_init:      CUDA3 KV buffer size =   109.38 MiB\n",
      "llama_kv_cache_init:      CUDA4 KV buffer size =   109.38 MiB\n",
      "llama_kv_cache_init:      CUDA5 KV buffer size =   109.38 MiB\n",
      "llama_kv_cache_init:      CUDA6 KV buffer size =   109.38 MiB\n",
      "llama_kv_cache_init:      CUDA7 KV buffer size =    93.75 MiB\n",
      "llama_new_context_with_model: KV self size  =  875.00 MiB, K (f16):  437.50 MiB, V (f16):  437.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA1 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA2 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA3 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA4 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA5 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA6 compute buffer size =   502.26 MiB\n",
      "llama_new_context_with_model:      CUDA7 compute buffer size =   502.27 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    43.27 MiB\n",
      "llama_new_context_with_model: graph nodes  = 2638\n",
      "llama_new_context_with_model: graph splits = 9\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'split.no': '0', 'tokenizer.chat_template': \"{{bos_token}}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ ' [INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + ' ' + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'split.count': '4', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '32768', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'false', 'llama.embedding_length': '6144', 'llama.feed_forward_length': '16384', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '48', 'llama.block_count': '56', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'split.tensors.count': '563', 'llama.context_length': '65536', 'general.name': 'models--mistralai--Mixtral-8x22B-Instruct-v0.1', 'llama.expert_used_count': '2', 'general.file_type': '17'}\n",
      "Using gguf chat template: {{bos_token}}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ ' [INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' ' + message['content'] + ' ' + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: </s>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_path= \"model/Mixtral-8x22B-Instruct-v0.1.Q5_K_M-00001-of-00004.gguf\"\n",
    "bot=GGUFBot(model_path,max_new_tokens=4000,n_ctx=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      25.24 ms /   117 runs   (    0.22 ms per token,  4635.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3144.47 ms /   168 tokens (   18.72 ms per token,    53.43 tokens per second)\n",
      "llama_print_timings:        eval time =    4485.93 ms /   116 runs   (   38.67 ms per token,    25.86 tokens per second)\n",
      "llama_print_timings:       total time =    7842.95 ms /   284 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =     125.25 ms /   612 runs   (    0.20 ms per token,  4886.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1123.40 ms /   113 tokens (    9.94 ms per token,   100.59 tokens per second)\n",
      "llama_print_timings:        eval time =   23797.06 ms /   611 runs   (   38.95 ms per token,    25.68 tokens per second)\n",
      "llama_print_timings:       total time =   26152.03 ms /   724 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      18.51 ms /    86 runs   (    0.22 ms per token,  4645.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3963.31 ms /   180 tokens (   22.02 ms per token,    45.42 tokens per second)\n",
      "llama_print_timings:        eval time =    3373.26 ms /    85 runs   (   39.69 ms per token,    25.20 tokens per second)\n",
      "llama_print_timings:       total time =    7487.99 ms /   265 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      58.30 ms /   279 runs   (    0.21 ms per token,  4785.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     927.11 ms /    82 tokens (   11.31 ms per token,    88.45 tokens per second)\n",
      "llama_print_timings:        eval time =   10651.43 ms /   278 runs   (   38.31 ms per token,    26.10 tokens per second)\n",
      "llama_print_timings:       total time =   12098.52 ms /   360 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      16.39 ms /    76 runs   (    0.22 ms per token,  4635.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1256.61 ms /   159 tokens (    7.90 ms per token,   126.53 tokens per second)\n",
      "llama_print_timings:        eval time =    2795.72 ms /    75 runs   (   37.28 ms per token,    26.83 tokens per second)\n",
      "llama_print_timings:       total time =    4187.14 ms /   234 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      90.85 ms /   435 runs   (    0.21 ms per token,  4788.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     899.73 ms /    72 tokens (   12.50 ms per token,    80.02 tokens per second)\n",
      "llama_print_timings:        eval time =   16799.84 ms /   434 runs   (   38.71 ms per token,    25.83 tokens per second)\n",
      "llama_print_timings:       total time =   18539.37 ms /   506 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      10.99 ms /    52 runs   (    0.21 ms per token,  4733.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1052.32 ms /   136 tokens (    7.74 ms per token,   129.24 tokens per second)\n",
      "llama_print_timings:        eval time =    1863.10 ms /    51 runs   (   36.53 ms per token,    27.37 tokens per second)\n",
      "llama_print_timings:       total time =    3006.36 ms /   187 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      64.22 ms /   311 runs   (    0.21 ms per token,  4843.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     778.93 ms /    48 tokens (   16.23 ms per token,    61.62 tokens per second)\n",
      "llama_print_timings:        eval time =   11939.03 ms /   310 runs   (   38.51 ms per token,    25.97 tokens per second)\n",
      "llama_print_timings:       total time =   13298.22 ms /   358 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      10.24 ms /    48 runs   (    0.21 ms per token,  4685.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1010.09 ms /   121 tokens (    8.35 ms per token,   119.79 tokens per second)\n",
      "llama_print_timings:        eval time =    1782.32 ms /    47 runs   (   37.92 ms per token,    26.37 tokens per second)\n",
      "llama_print_timings:       total time =    2876.91 ms /   168 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      98.42 ms /   465 runs   (    0.21 ms per token,  4724.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     868.64 ms /    44 tokens (   19.74 ms per token,    50.65 tokens per second)\n",
      "llama_print_timings:        eval time =   17843.01 ms /   464 runs   (   38.45 ms per token,    26.00 tokens per second)\n",
      "llama_print_timings:       total time =   19648.13 ms /   508 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =       4.32 ms /    21 runs   (    0.21 ms per token,  4858.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =     981.30 ms /   102 tokens (    9.62 ms per token,   103.94 tokens per second)\n",
      "llama_print_timings:        eval time =     740.14 ms /    20 runs   (   37.01 ms per token,    27.02 tokens per second)\n",
      "llama_print_timings:       total time =    1758.26 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      13.06 ms /    66 runs   (    0.20 ms per token,  5053.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =     354.51 ms /    17 tokens (   20.85 ms per token,    47.95 tokens per second)\n",
      "llama_print_timings:        eval time =    2524.55 ms /    65 runs   (   38.84 ms per token,    25.75 tokens per second)\n",
      "llama_print_timings:       total time =    2995.20 ms /    82 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      56.41 ms /   265 runs   (    0.21 ms per token,  4697.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     977.65 ms /    95 tokens (   10.29 ms per token,    97.17 tokens per second)\n",
      "llama_print_timings:        eval time =   10213.55 ms /   264 runs   (   38.69 ms per token,    25.85 tokens per second)\n",
      "llama_print_timings:       total time =   11683.87 ms /   359 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =     133.43 ms /   612 runs   (    0.22 ms per token,  4586.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5156.13 ms /   264 tokens (   19.53 ms per token,    51.20 tokens per second)\n",
      "llama_print_timings:        eval time =   23876.46 ms /   611 runs   (   39.08 ms per token,    25.59 tokens per second)\n",
      "llama_print_timings:       total time =   30287.40 ms /   875 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      34.10 ms /   158 runs   (    0.22 ms per token,  4633.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =     936.46 ms /    94 tokens (    9.96 ms per token,   100.38 tokens per second)\n",
      "llama_print_timings:        eval time =    6068.65 ms /   157 runs   (   38.65 ms per token,    25.87 tokens per second)\n",
      "llama_print_timings:       total time =    7293.10 ms /   251 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =     182.96 ms /   842 runs   (    0.22 ms per token,  4602.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1061.96 ms /   154 tokens (    6.90 ms per token,   145.02 tokens per second)\n",
      "llama_print_timings:        eval time =   32848.58 ms /   841 runs   (   39.06 ms per token,    25.60 tokens per second)\n",
      "llama_print_timings:       total time =   35737.28 ms /   995 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      18.50 ms /    83 runs   (    0.22 ms per token,  4486.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1129.60 ms /   159 tokens (    7.10 ms per token,   140.76 tokens per second)\n",
      "llama_print_timings:        eval time =    3172.66 ms /    82 runs   (   38.69 ms per token,    25.85 tokens per second)\n",
      "llama_print_timings:       total time =    4454.08 ms /   241 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =     155.71 ms /   714 runs   (    0.22 ms per token,  4585.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1101.94 ms /    79 tokens (   13.95 ms per token,    71.69 tokens per second)\n",
      "llama_print_timings:        eval time =   27624.08 ms /   713 runs   (   38.74 ms per token,    25.81 tokens per second)\n",
      "llama_print_timings:       total time =   30228.48 ms /   792 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =       3.02 ms /    14 runs   (    0.22 ms per token,  4628.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     968.35 ms /   105 tokens (    9.22 ms per token,   108.43 tokens per second)\n",
      "llama_print_timings:        eval time =     478.96 ms /    13 runs   (   36.84 ms per token,    27.14 tokens per second)\n",
      "llama_print_timings:       total time =    1472.73 ms /   118 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      38.06 ms /   183 runs   (    0.21 ms per token,  4807.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     276.96 ms /    13 tokens (   21.30 ms per token,    46.94 tokens per second)\n",
      "llama_print_timings:        eval time =    7000.04 ms /   182 runs   (   38.46 ms per token,    26.00 tokens per second)\n",
      "llama_print_timings:       total time =    7617.19 ms /   195 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =       4.16 ms /    20 runs   (    0.21 ms per token,  4805.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     932.63 ms /   103 tokens (    9.05 ms per token,   110.44 tokens per second)\n",
      "llama_print_timings:        eval time =     695.78 ms /    19 runs   (   36.62 ms per token,    27.31 tokens per second)\n",
      "llama_print_timings:       total time =    1664.90 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      57.41 ms /   275 runs   (    0.21 ms per token,  4789.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     334.91 ms /    16 tokens (   20.93 ms per token,    47.77 tokens per second)\n",
      "llama_print_timings:        eval time =   10581.47 ms /   274 runs   (   38.62 ms per token,    25.89 tokens per second)\n",
      "llama_print_timings:       total time =   11438.42 ms /   290 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =       3.18 ms /    15 runs   (    0.21 ms per token,  4724.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     978.95 ms /    97 tokens (   10.09 ms per token,    99.09 tokens per second)\n",
      "llama_print_timings:        eval time =     505.17 ms /    14 runs   (   36.08 ms per token,    27.71 tokens per second)\n",
      "llama_print_timings:       total time =    1511.26 ms /   111 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      18.75 ms /    91 runs   (    0.21 ms per token,  4852.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     291.53 ms /    14 tokens (   20.82 ms per token,    48.02 tokens per second)\n",
      "llama_print_timings:        eval time =    3559.03 ms /    90 runs   (   39.54 ms per token,    25.29 tokens per second)\n",
      "llama_print_timings:       total time =    4017.42 ms /   104 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =       5.63 ms /    27 runs   (    0.21 ms per token,  4794.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     932.02 ms /    99 tokens (    9.41 ms per token,   106.22 tokens per second)\n",
      "llama_print_timings:        eval time =     945.40 ms /    26 runs   (   36.36 ms per token,    27.50 tokens per second)\n",
      "llama_print_timings:       total time =    1924.95 ms /   125 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      50.58 ms /   235 runs   (    0.22 ms per token,  4646.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =     443.61 ms /    23 tokens (   19.29 ms per token,    51.85 tokens per second)\n",
      "llama_print_timings:        eval time =    9012.70 ms /   234 runs   (   38.52 ms per token,    25.96 tokens per second)\n",
      "llama_print_timings:       total time =    9899.16 ms /   257 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =       8.75 ms /    41 runs   (    0.21 ms per token,  4685.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1139.63 ms /   112 tokens (   10.18 ms per token,    98.28 tokens per second)\n",
      "llama_print_timings:        eval time =    1445.34 ms /    40 runs   (   36.13 ms per token,    27.68 tokens per second)\n",
      "llama_print_timings:       total time =    2659.90 ms /   152 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =     106.76 ms /   495 runs   (    0.22 ms per token,  4636.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     685.51 ms /    37 tokens (   18.53 ms per token,    53.97 tokens per second)\n",
      "llama_print_timings:        eval time =   19099.12 ms /   494 runs   (   38.66 ms per token,    25.87 tokens per second)\n",
      "llama_print_timings:       total time =   20778.74 ms /   531 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      22.45 ms /   103 runs   (    0.22 ms per token,  4587.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =     947.37 ms /   111 tokens (    8.53 ms per token,   117.17 tokens per second)\n",
      "llama_print_timings:        eval time =    3897.24 ms /   102 runs   (   38.21 ms per token,    26.17 tokens per second)\n",
      "llama_print_timings:       total time =    5032.77 ms /   213 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =     186.78 ms /   879 runs   (    0.21 ms per token,  4705.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1138.97 ms /    99 tokens (   11.50 ms per token,    86.92 tokens per second)\n",
      "llama_print_timings:        eval time =   34131.41 ms /   878 runs   (   38.87 ms per token,    25.72 tokens per second)\n",
      "llama_print_timings:       total time =   37193.81 ms /   977 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =       5.65 ms /    27 runs   (    0.21 ms per token,  4776.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     956.02 ms /    96 tokens (    9.96 ms per token,   100.42 tokens per second)\n",
      "llama_print_timings:        eval time =     937.19 ms /    26 runs   (   36.05 ms per token,    27.74 tokens per second)\n",
      "llama_print_timings:       total time =    1942.86 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      19.89 ms /    96 runs   (    0.21 ms per token,  4827.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     441.45 ms /    23 tokens (   19.19 ms per token,    52.10 tokens per second)\n",
      "llama_print_timings:        eval time =    3766.99 ms /    95 runs   (   39.65 ms per token,    25.22 tokens per second)\n",
      "llama_print_timings:       total time =    4383.16 ms /   118 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      13.15 ms /    64 runs   (    0.21 ms per token,  4866.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =     974.81 ms /   104 tokens (    9.37 ms per token,   106.69 tokens per second)\n",
      "llama_print_timings:        eval time =    2286.62 ms /    63 runs   (   36.30 ms per token,    27.55 tokens per second)\n",
      "llama_print_timings:       total time =    3376.68 ms /   167 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      15.54 ms /    76 runs   (    0.20 ms per token,  4889.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     957.90 ms /    63 tokens (   15.20 ms per token,    65.77 tokens per second)\n",
      "llama_print_timings:        eval time =    2951.28 ms /    75 runs   (   39.35 ms per token,    25.41 tokens per second)\n",
      "llama_print_timings:       total time =    4046.53 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      44.79 ms /   206 runs   (    0.22 ms per token,  4599.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     931.71 ms /    92 tokens (   10.13 ms per token,    98.74 tokens per second)\n",
      "llama_print_timings:        eval time =    7835.16 ms /   205 runs   (   38.22 ms per token,    26.16 tokens per second)\n",
      "llama_print_timings:       total time =    9154.77 ms /   297 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      73.64 ms /   350 runs   (    0.21 ms per token,  4752.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1191.15 ms /   205 tokens (    5.81 ms per token,   172.10 tokens per second)\n",
      "llama_print_timings:        eval time =   13419.08 ms /   349 runs   (   38.45 ms per token,    26.01 tokens per second)\n",
      "llama_print_timings:       total time =   15287.81 ms /   554 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =       7.25 ms /    33 runs   (    0.22 ms per token,  4548.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1190.93 ms /   160 tokens (    7.44 ms per token,   134.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1168.53 ms /    32 runs   (   36.52 ms per token,    27.38 tokens per second)\n",
      "llama_print_timings:       total time =    2419.61 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      12.45 ms /    60 runs   (    0.21 ms per token,  4818.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =     734.00 ms /    32 tokens (   22.94 ms per token,    43.60 tokens per second)\n",
      "llama_print_timings:        eval time =    2217.06 ms /    59 runs   (   37.58 ms per token,    26.61 tokens per second)\n",
      "llama_print_timings:       total time =    3063.57 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =       7.41 ms /    36 runs   (    0.21 ms per token,  4861.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1009.34 ms /   115 tokens (    8.78 ms per token,   113.94 tokens per second)\n",
      "llama_print_timings:        eval time =    1277.63 ms /    35 runs   (   36.50 ms per token,    27.39 tokens per second)\n",
      "llama_print_timings:       total time =    2351.41 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =       5.83 ms /    29 runs   (    0.20 ms per token,  4971.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     802.78 ms /    36 tokens (   22.30 ms per token,    44.84 tokens per second)\n",
      "llama_print_timings:        eval time =    1087.52 ms /    28 runs   (   38.84 ms per token,    25.75 tokens per second)\n",
      "llama_print_timings:       total time =    1941.90 ms /    64 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =       6.45 ms /    32 runs   (    0.20 ms per token,  4959.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1075.48 ms /   106 tokens (   10.15 ms per token,    98.56 tokens per second)\n",
      "llama_print_timings:        eval time =    1121.97 ms /    31 runs   (   36.19 ms per token,    27.63 tokens per second)\n",
      "llama_print_timings:       total time =    2254.52 ms /   137 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      19.16 ms /    92 runs   (    0.21 ms per token,  4802.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     493.85 ms /    25 tokens (   19.75 ms per token,    50.62 tokens per second)\n",
      "llama_print_timings:        eval time =    3495.59 ms /    91 runs   (   38.41 ms per token,    26.03 tokens per second)\n",
      "llama_print_timings:       total time =    4156.63 ms /   116 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =       7.71 ms /    36 runs   (    0.21 ms per token,  4669.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1156.06 ms /   110 tokens (   10.51 ms per token,    95.15 tokens per second)\n",
      "llama_print_timings:        eval time =    1283.68 ms /    35 runs   (   36.68 ms per token,    27.27 tokens per second)\n",
      "llama_print_timings:       total time =    2505.13 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      16.04 ms /    79 runs   (    0.20 ms per token,  4926.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     637.77 ms /    32 tokens (   19.93 ms per token,    50.17 tokens per second)\n",
      "llama_print_timings:        eval time =    2978.90 ms /    78 runs   (   38.19 ms per token,    26.18 tokens per second)\n",
      "llama_print_timings:       total time =    3760.53 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3145.09 ms\n",
      "llama_print_timings:      sample time =      17.64 ms /    84 runs   (    0.21 ms per token,  4762.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1108.28 ms /    98 tokens (   11.31 ms per token,    88.43 tokens per second)\n",
      "llama_print_timings:        eval time =    3126.68 ms /    83 runs   (   37.67 ms per token,    26.55 tokens per second)\n",
      "llama_print_timings:       total time =    4387.42 ms /   181 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "out_path=\"data/t1.jsonl\"\n",
    "\n",
    "question=\"あなたはサイズmとnの2つのソートリストを与えられます。二つのリストの合計からk番目に小さい要素を見つける関数を線形の複雑度で実装してください。\"\n",
    "\n",
    "question_template = f\"\"\"以下の問題の類題を作成してください。\n",
    "・類題はもとの問題からは必ず情報を追加・修正・削除し、内容、形式、記述方式が全く異なるようにすること\n",
    "・問題文のみを出力すること\n",
    "\n",
    "#問題\n",
    "\"\"\"\n",
    "while True:\n",
    "    q=question_template+question\n",
    "    try:\n",
    "        new_question=bot.ask(q).replace(\"#類題\",\"\").replace(\"#問題\",\"\").strip()[:3000]\n",
    "        ans=bot.ask(new_question)\n",
    "        d={\"question\":new_question,\"answer\":ans}\n",
    "        with open(out_path, 'a') as f:\n",
    "            f.write(json.dumps(d,ensure_ascii=False)+\"\\n\")\n",
    "    except:\n",
    "        pass\n",
    "    question=new_question\n",
    "    #question=question[:random.randint(0,len(question))]\n",
    "    if random.randint(0,3)==0:\n",
    "        question=question[:int(len(question)*0.8)]\n",
    "    elif random.randint(0,3)==1:\n",
    "        question=question[int(len(question)*0.2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
